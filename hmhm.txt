ASSIGN 4 - HIVE TABLE AND PYSPARK


# Step 1: Initialize SparkSession for reading CSV into DataFrame
spark = SparkSession.builder.appName("ReadCSV").enableHiveSupport().getOrCreate()
df = spark.read.csv("ds_salaries.csv", header=True, inferSchema=True)
df.show()
df.printSchema()


import os
os.environ["HADOOP_HOME"] = "D:\MIT ADT\LY - Sem 1\BDA Lab\Sir\Assign 4\hadoop-3.4.0-src" 
spark.sql("CREATE DATABASE IF NOT EXISTS mydb")
# Step 3: Create and Query a Hive Table in PySpark

# Create a new Hive database (if it doesn't exist) and use it

spark.sql("USE mydb")
# Create a Hive table
spark.sql("""
CREATE TABLE IF NOT EXISTS my_table (
    id INT,
    name STRING,
    age INT
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
""")
# Load data into the Hive table
spark.sql("LOAD DATA LOCAL INPATH 'path/to/your/csvfile.csv' INTO TABLE my_table")

# Query the Hive table and display results
result = spark.sql("SELECT * FROM my_table")
result.show()


ASSIGNMENT 5 - Kmeans

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Sir\\Assign 5")

# Install the required libraries if not already installed
install.packages("ggplot2")
install.packages("factoextra")
install.packages("dplyr")

install.packages("colorspace")
# Load the required libraries
library(ggplot2)
library(factoextra)
library(dplyr)

#Load the Mall Customers Dataset
mall_customers = read.csv("Mall_Customers.csv")
head(mall_customers)

names(mall_customers)
print(colnames(mall_customers))

data <- mall_customers[c("Annual.Income","Spending.Score")]
head(data)

# Scale the data (optional, but recommended for K-Means)
data_scaled <- scale(data)
print("scaled data")
head(data_scaled)
# calculated WSS for each clustes from 1 to 15
wss <- numeric(15)
for (k in 1:15) wss[k] <- sum(kmeans(data_scaled, centers=k,
                                     nstart=25)$withinss)

# plot the graph of number of clusters vs WSS
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="WSS")
#Plots both points and lines. The points are plotted at each data value, and theyâ€™re connected by lines.
# Elbow method to find the optimal number of clusters
fviz_nbclust(data_scaled, kmeans, method = "wss") + 
  geom_vline(xintercept = 5, linetype = 2) +
  labs(subtitle = "Elbow method")


# Silhouette method
fviz_nbclust(data_scaled, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
  
# Apply K-Means clustering with 3 clusters
set.seed(123)  # Set seed for reproducibility in case python- random_state
kmeans_result <- kmeans(data_scaled, centers = 5, nstart = 25)

# Add the cluster assignments to the original dataset
mall_customers$Cluster <- as.factor(kmeans_result$cluster)
tail(mall_customers)

# Scatter plot of the clusters
ggplot(mall_customers, aes(x = Annual.Income, y = Spending.Score, color = Cluster)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green","yellow","black")) +
  labs(title = "K-Means Clustering of Mall Customers",
       x = "Annual Income (k$)",
       y = "Spending Score (1-100)") +
  theme_minimal()

# Scatter plot with cluster centers
fviz_cluster(kmeans_result, data = data_scaled,
             geom = "point",
             ellipse.type = "norm",
             ggtheme = theme_minimal(),
             palette = c("red", "blue", "green","yellow","black"))
kmeans_result





ASSIGN - 6 TF IDF IPYNB

import nltk
from nltk.corpus import stopwords
from math import log
import pandas as pd
# Download stopwords list if you haven't
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))
# Sample corpus
corpus = [
    "The car is driven on the road",
    "The truck is driven on the highway",
    "The car is blue",
    "The car is fast and driven on the road",
    "I like the blue car"
]


# Step 1: Tokenization with stop word removal
def tokenize(doc):
    # Tokenize and remove stop words
    return [word.lower() for word in doc.split() if word.lower() not in stop_words]

# Step 2: Create vocabulary (all unique terms excluding stop words)
vocabulary = list(set(word for doc in corpus for word in tokenize(doc)))

# Step 3: Calculate Term Frequency (TF) for all terms in each document
def term_frequency_matrix(corpus, vocabulary):
    tf_matrix = []
    for doc in corpus:
        tf_vector = []
        doc_tokens = tokenize(doc)
        for term in vocabulary:
            tf = doc_tokens.count(term) / len(doc_tokens)  # Relative frequency
            tf_vector.append(tf)
        tf_matrix.append(tf_vector)
    return tf_matrix

# Step 4: Calculate Document Frequency (DF) for each term
def document_frequency(term, corpus):
    return sum(1 for doc in corpus if term in tokenize(doc))

# Step 5: Calculate Inverse Document Frequency (IDF) for each term
def inverse_document_frequency_matrix(corpus, vocabulary):
    N = len(corpus)
    idf_vector = []
    for term in vocabulary:
        df = document_frequency(term, corpus)
        idf = log(N / (df))  # Adding 1 to avoid division by zero
        idf_vector.append(idf)
    return idf_vector

# Step 6: Calculate TF-IDF matrix by multiplying TF and IDF
def tfidf_matrix(tf_matrix, idf_vector):
    tfidf_matrix = []
    for tf_vector in tf_matrix:
        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]
        tfidf_matrix.append(tfidf_vector)
    return tfidf_matrix

# Step 7: Compute TF, IDF, and TF-IDF
tf_matrix = term_frequency_matrix(corpus, vocabulary)
idf_vector = inverse_document_frequency_matrix(corpus, vocabulary)
tfidf = tfidf_matrix(tf_matrix, idf_vector)

# Step 8: Convert to DataFrames for display
df_tf = pd.DataFrame(tf_matrix, columns=vocabulary)
df_idf = pd.DataFrame([idf_vector], columns=vocabulary)
df_tfidf = pd.DataFrame(tfidf, columns=vocabulary)

# Step 9: Display TF, IDF, and TF-IDF
print("Term Frequency (TF):")
print(df_tf)

print("\nInverse Document Frequency (IDF):")
print(df_idf)

print("\nTF-IDF:")
print(df_tfidf)



ASSIGN 7 - LINEAR REGRESSION


movies_data <- data.frame(
  MovieID = 1:10,
  Genre = factor(c("Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action")),
  Budget = c(15000000, 5000000, 10000000, 20000000, 6000000, 12000000, 18000000, 5500000, 11000000, 16000000),
  Rating = c(7.8, 6.5, 8.2, 7.9, 6.9, 8.0, 7.5, 6.7, 8.1, 7.7)
)

write.csv(movies_data, file = "movies_data.csv", row.names = FALSE)

print(movies_data)

install.packages("ggplot2")
install.packages("dplyr")

library(ggplot2)
library(dplyr)


head(movies_data)

str(movies_data)

movies_data$Genre <- as.factor(movies_data$Genre)

summary(movies_data)

set.seed(123)

# Create a training set (70%) and test set (30%)
sample_indices <- sample(seq_len(nrow(movies_data)), size = 0.7 * nrow(movies_data))
train_set <- movies_data[sample_indices, ]
test_set <- movies_data[-sample_indices, ]

model <- lm(Rating ~ Genre + Budget, data = train_set)

# Summary of the model
summary(model)

predictions <- predict(model, newdata = test_set)

# Combine predictions with actual values
results <- data.frame(Actual = test_set$Rating, Predicted = predictions)

# Calculate Mean Squared Error
mse <- mean((results$Actual - results$Predicted)^2)
print(paste("Mean Squared Error:", mse))


ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Actual vs Predicted Ratings", x = "Actual Rating", y = "Predicted Rating")


residuals <- results$Actual - results$Predicted

ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency")



ASSIGN 8 - KAFKA VIDEO

https://youtu.be/oI7VAS9KSS4?si=SWYz0WHTmWSTsArS


ASSIGN 9 - SOCIAL NETWORK ANALYSIS


install.packages("igraph")
install.packages("sna")

library(igraph)
library(network)
library(sna)

plot(make_full_graph(8, directed=FALSE))

Ring_Graph <- make_ring(12, directed = FALSE, mutual =FALSE, circular = TRUE)
# try for False
plot(Ring_Graph)

Star_Graph <- make_star(12, center = 4)
plot(Star_Graph)

gnp_Graph <- sample_gnp(20, 0.5, directed = FALSE, loops =FALSE)
plot(gnp_Graph)

gnp_Graph1 <- sample_gnp(7, 0.4, directed = FALSE, loops = FALSE)
plot(gnp_Graph1)

node_degrees <- degree(gnp_Graph)
print(node_degrees)

sample_graph <- sample_gnp(10, 0.3, directed = FALSE)
plot(sample_graph)
sample_density <- edge_density(sample_graph, loops = FALSE)
sample_density

sample_graph <- sample_gnp(20, 0.3, directed = FALSE, loops= FALSE)
plot(sample_graph)
clique_num(sample_graph)

components(sample_graph)


setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 9")
data <- read.csv("socialnetworkdata.csv")
y <- data.frame(data$first, data$second)
net <- graph.data.frame(y, directed=T)
V(net)
E(net)
plot(net)

hist(degree(net), col='purple', main = "histogram of node degree",ylab = 'freq', xlab = 'vertices degree')


set.seed(222)
plot(net,
     vertex.color = 'cyan',
     vertext.size = 2,
     edge.arrow.size = 0.1,
     vertex.label.cex = 0.8)


plot(net,
     vertex.color = rainbow(vcount(net)),  # Use the number of vertices to generate colors
     vertex.size = degree(net) * 0.4,      # Calculate vertex degrees and scale their size
     edge.arrow.size = 0.1,                # Keep arrow size for edges
     layout = layout.fruchterman.reingold)


hs <- hub_score(net)$vector
hs
as <- authority_score(net)$vector
as
set.seed(123)
plot(net,
     vertex.size=hs*30,
     main = 'Hubs',
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai)


undirected_net <- as.undirected(net, mode = "collapse")

community <- cluster_louvain(undirected_net)

plot(undirected_net,
     vertex.color = membership(community),  # Color vertices by community membership
     vertex.size = degree(undirected_net) * 0.4,  # Size vertices by degree
     edge.arrow.size = 0.1,  # Edge arrow size
     main = "Community Detection using Louvain Method")





